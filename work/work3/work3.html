<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Project-FacialPen</title>
	<!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <!-- =====BOX ICONS===== -->
    <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
	<link rel="stylesheet" href="work3Style.css">
</head>
<body>
	<header>
		<div class="banner">
				<img class="sketchImg" src="imgs/banner.png" alt="">
				<a href="../../index.html" class="home__social-icon"><i class='bx bx-x'></i></i></a>
			</div>
		<h1 style="margin-bottom: 1rem;">FacialPen</h1>
		<h2 style="margin-bottom: 3rem;">Using Facial Detection to Augment Pen-Based Interaction</h2>
	</header>

	<main>
		<section class="introduction">
			<div class="intro-content">
				<div class="intro-item">
					<span class="intro-title">ROLE</span>
					<p>HCI Research</p>
				</div>
				<div class="intro-item">
					<span class="intro-title">DURATION</span>
					<p>Apr 2020--Present</p>
				</div>
				<div class="intro-item">
					<span class="intro-title">TOOLS</span>
					<p>Python, JavaScript, CSS, WebSockets</p>
				</div>
				<div class="intro-item">
					<span class="intro-title">TEAM</span>
					<p>Cooperated with PHD.Chengshuo Xia, Supervised by Yuta Sugiura</p>
				</div>
			</div>
		</section>

		<section>
			<h2>Introduction</h2>
			<h3>Submitted to CHI 2021, under review</h3>
			<p class="p-margin">
				Pen-based interaction has been ubiquitously adopted on mobile and stationary devices, but its usability can be further augmented through use of advanced techniques. In this work, we propose FacialPen, a prototype that uses facial gestures to trigger commands for pen-based manipulation. In our prototype, a camera is mounted to the end of a stylus that provides a broad view from which to capture the human face. We facilitated <span class="userStudy">an elicitation study</span> to identify natural and user-defined gestures for interaction with facial expression. Different gestures can be further discerned via a CNN (Convolutional Neural Network) classification and a calibration pipeline. We designed three demonstration applications to explore usage scenarios and evaluated the effectiveness of FacialPen with <span class="userStudy">a qualitative study</span>. The user study posits that FacialPen supports efficiency by reducing screen widgets, liberating the userâ€™s non-dominant hand, and enabling the dominant hand to focus on work.
			</p>
			<p>
				Upload later when receiving feedback from reviewers...
			</p>
		</section>

		
	</main>
	
	<!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>

</body>
</html>